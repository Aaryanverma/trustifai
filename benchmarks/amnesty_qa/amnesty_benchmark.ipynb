{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a628774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from trustifai import Trustifai, MetricContext\n",
    "from datasets import load_dataset\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../creds.env\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "493b91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppress pydantic warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf0b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"vibrantlabsai/amnesty_qa\", \"english_v3\", split=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e4240be",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP_ORDINAL = {\n",
    "\"UNRELIABLE\": 0,\n",
    "\"ACCEPTABLE (WITH CAUTION)\": 1,\n",
    "\"RELIABLE\": 2,\n",
    "}\n",
    "\n",
    "LABEL_MAP_BINARY = {\n",
    "\"UNRELIABLE\": 0,\n",
    "\"ACCEPTABLE (WITH CAUTION)\": 1,\n",
    "\"RELIABLE\": 1,\n",
    "}\n",
    "\n",
    "def prepare_labels(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    df[\"response_label_ordinal\"] = df[\"response_label\"].map(LABEL_MAP_ORDINAL)\n",
    "    df[\"response_label_binary\"] = df[\"response_label\"].map(LABEL_MAP_BINARY)\n",
    "    df['ground_truth_label_ordinal'] = df[\"ground_truth_label\"].map(LABEL_MAP_ORDINAL)\n",
    "    df['ground_truth_label_binary'] = df[\"ground_truth_label\"].map(LABEL_MAP_BINARY)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2e93f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ccb80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "sem = asyncio.Semaphore(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1646be",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = Trustifai(\"../config_file.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa7111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_row_sync(row):\n",
    "    context = MetricContext(\n",
    "        query=row[\"user_input\"],\n",
    "        answer=row[\"response\"],\n",
    "        documents=row[\"retrieved_contexts\"],\n",
    "    )\n",
    "    return engine.get_trust_score(context)\n",
    "\n",
    "async def score_row(row):\n",
    "    async with sem:\n",
    "        result = await asyncio.to_thread(score_row_sync, row)\n",
    "        return result[\"score\"], result[\"label\"]\n",
    "\n",
    "tasks = [score_row(row) for row in dataset]\n",
    "results = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "response_scores, response_labels = zip(*results)\n",
    "response_scores = list(response_scores)\n",
    "response_labels = list(response_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98312ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_ground_truth_row_sync(row):\n",
    "    context = MetricContext(\n",
    "        query=row[\"user_input\"],\n",
    "        answer=row[\"reference\"],\n",
    "        documents=row[\"retrieved_contexts\"],\n",
    "    )\n",
    "    return engine.get_trust_score(context)\n",
    "\n",
    "async def score_row(row):\n",
    "    async with sem:\n",
    "        result = await asyncio.to_thread(score_ground_truth_row_sync, row)\n",
    "        return result[\"score\"], result[\"label\"]\n",
    "\n",
    "tasks = [score_row(row) for row in dataset]\n",
    "results = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "ground_scores, ground_labels = zip(*results)\n",
    "ground_scores = list(ground_scores)\n",
    "ground_labels = list(ground_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4703ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['response_score'] = response_scores\n",
    "df['response_label'] = response_labels\n",
    "df['ground_truth_score'] = ground_scores\n",
    "df['ground_truth_label'] = ground_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "961dda53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68f81db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference</th>\n",
       "      <th>response</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response_score</th>\n",
       "      <th>response_label</th>\n",
       "      <th>ground_truth_score</th>\n",
       "      <th>ground_truth_label</th>\n",
       "      <th>response_label_ordinal</th>\n",
       "      <th>response_label_binary</th>\n",
       "      <th>ground_truth_label_ordinal</th>\n",
       "      <th>ground_truth_label_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the global implications of the USA Su...</td>\n",
       "      <td>The global implications of the USA Supreme Cou...</td>\n",
       "      <td>The global implications of the USA Supreme Cou...</td>\n",
       "      <td>[- In 2022, the USA Supreme Court handed down ...</td>\n",
       "      <td>0.83</td>\n",
       "      <td>RELIABLE</td>\n",
       "      <td>0.94</td>\n",
       "      <td>RELIABLE</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which companies are the main contributors to G...</td>\n",
       "      <td>According to the Carbon Majors database, the m...</td>\n",
       "      <td>According to the Carbon Majors database, the m...</td>\n",
       "      <td>[In recent years, there has been increasing pr...</td>\n",
       "      <td>0.63</td>\n",
       "      <td>ACCEPTABLE (WITH CAUTION)</td>\n",
       "      <td>0.91</td>\n",
       "      <td>RELIABLE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which private companies in the Americas are th...</td>\n",
       "      <td>The largest private companies in the Americas ...</td>\n",
       "      <td>According to the Carbon Majors database, the l...</td>\n",
       "      <td>[The issue of greenhouse gas emissions has bec...</td>\n",
       "      <td>0.61</td>\n",
       "      <td>ACCEPTABLE (WITH CAUTION)</td>\n",
       "      <td>0.90</td>\n",
       "      <td>RELIABLE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What action did Amnesty International urge its...</td>\n",
       "      <td>Amnesty International urged its supporters to ...</td>\n",
       "      <td>Amnesty International urged its supporters to ...</td>\n",
       "      <td>[In the case of the Ogoni 9, Amnesty Internati...</td>\n",
       "      <td>0.69</td>\n",
       "      <td>ACCEPTABLE (WITH CAUTION)</td>\n",
       "      <td>0.84</td>\n",
       "      <td>RELIABLE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the recommendations made by Amnesty I...</td>\n",
       "      <td>The recommendations made by Amnesty Internatio...</td>\n",
       "      <td>Amnesty International made several recommendat...</td>\n",
       "      <td>[In recent years, Amnesty International has fo...</td>\n",
       "      <td>0.52</td>\n",
       "      <td>UNRELIABLE</td>\n",
       "      <td>0.92</td>\n",
       "      <td>RELIABLE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What are the global implications of the USA Su...   \n",
       "1  Which companies are the main contributors to G...   \n",
       "2  Which private companies in the Americas are th...   \n",
       "3  What action did Amnesty International urge its...   \n",
       "4  What are the recommendations made by Amnesty I...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  The global implications of the USA Supreme Cou...   \n",
       "1  According to the Carbon Majors database, the m...   \n",
       "2  The largest private companies in the Americas ...   \n",
       "3  Amnesty International urged its supporters to ...   \n",
       "4  The recommendations made by Amnesty Internatio...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The global implications of the USA Supreme Cou...   \n",
       "1  According to the Carbon Majors database, the m...   \n",
       "2  According to the Carbon Majors database, the l...   \n",
       "3  Amnesty International urged its supporters to ...   \n",
       "4  Amnesty International made several recommendat...   \n",
       "\n",
       "                                  retrieved_contexts  response_score  \\\n",
       "0  [- In 2022, the USA Supreme Court handed down ...            0.83   \n",
       "1  [In recent years, there has been increasing pr...            0.63   \n",
       "2  [The issue of greenhouse gas emissions has bec...            0.61   \n",
       "3  [In the case of the Ogoni 9, Amnesty Internati...            0.69   \n",
       "4  [In recent years, Amnesty International has fo...            0.52   \n",
       "\n",
       "              response_label  ground_truth_score ground_truth_label  \\\n",
       "0                   RELIABLE                0.94           RELIABLE   \n",
       "1  ACCEPTABLE (WITH CAUTION)                0.91           RELIABLE   \n",
       "2  ACCEPTABLE (WITH CAUTION)                0.90           RELIABLE   \n",
       "3  ACCEPTABLE (WITH CAUTION)                0.84           RELIABLE   \n",
       "4                 UNRELIABLE                0.92           RELIABLE   \n",
       "\n",
       "   response_label_ordinal  response_label_binary  ground_truth_label_ordinal  \\\n",
       "0                       2                      1                           2   \n",
       "1                       1                      1                           2   \n",
       "2                       1                      1                           2   \n",
       "3                       1                      1                           2   \n",
       "4                       0                      0                           2   \n",
       "\n",
       "   ground_truth_label_binary  \n",
       "0                          1  \n",
       "1                          1  \n",
       "2                          1  \n",
       "3                          1  \n",
       "4                          1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0248583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"benchmark_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(df: pd.DataFrame):\n",
    "    if df is None:\n",
    "        raise RuntimeError(\"Run evaluation first\")\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    def safe_auc(y, s):\n",
    "        return roc_auc_score(y, s) if len(set(y)) > 1 else None\n",
    "\n",
    "    def safe_pr(y, s):\n",
    "        return average_precision_score(y, s) if len(set(y)) > 1 else None\n",
    "\n",
    "    # Binary detection\n",
    "    metrics[\"response_roc_auc\"] = safe_auc(\n",
    "        df[\"response_label_binary\"], df[\"response_score\"]\n",
    "    )\n",
    "    metrics[\"response_pr_auc\"] = safe_pr(\n",
    "        df[\"response_label_binary\"], df[\"response_score\"]\n",
    "    )\n",
    "\n",
    "    # Ordinal calibration\n",
    "    metrics[\"response_spearman\"] = spearmanr(\n",
    "        df[\"response_label_ordinal\"], df[\"response_score\"]\n",
    "    ).correlation\n",
    "\n",
    "    metrics[\"response_pearson\"] = pearsonr(\n",
    "        df[\"response_label_ordinal\"], df[\"response_score\"]\n",
    "    )[0]\n",
    "\n",
    "    # Distribution comparison\n",
    "    distribution_df = pd.DataFrame({\n",
    "        'Label': list(df[\"response_label\"]) + list(df[\"ground_truth_label\"]),\n",
    "        'Type': [\"LLM\"] * len(df) + [\"Ground Truth\"] * len(df)\n",
    "    })\n",
    "\n",
    "    metrics[\"label_distribution\"] = (\n",
    "        distribution_df.groupby(\"Label\")\n",
    "            .value_counts()\n",
    "            .unstack(fill_value=0).T\n",
    "    )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def generate_report(\n",
    "    metrics: dict,\n",
    "    export_path: str = \"benchmark_report.md\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a TrustifAI benchmark report (Markdown),\n",
    "    strictly aligned with the expected reference format.\n",
    "    \"\"\"\n",
    "\n",
    "    def fmt(x):\n",
    "        return \"N/A\" if x is None else f\"{x:.3f}\"\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Header\n",
    "    # --------------------------------------------------\n",
    "    lines.append(\"# TrustifAI Benchmark Report\\n\")\n",
    "    lines.append(f\"**Generated on:** {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Dataset Details\n",
    "    # --------------------------------------------------\n",
    "    lines.append(\"## Dataset Details\\n\")\n",
    "    lines.append(\n",
    "        \"This benchmark is conducted using the \"\n",
    "        \"[vibrantlabsai/amnesty_qa dataset](https://huggingface.co/datasets/vibrantlabsai/amnesty_qa) from huggingface \"\n",
    "        \"which contains question-answer pairs related to human rights \"\n",
    "        \"and Amnesty International reports. The dataset includes:\\n\\n\"\n",
    "        \"- 20 ground-truth answers sourced directly from verified Amnesty International documents\\n\"\n",
    "        \"- 20 LLM-generated answers produced by querying language models\\n\"\n",
    "        \"- Total of 40 QA pairs evaluated\\n\\n\"\n",
    "        \"The ground truth answers serve as a reliable baseline, while the LLM \"\n",
    "        \"answers help assess TrustifAI's ability to detect potential \"\n",
    "        \"hallucinations and inaccuracies in model-generated content.\\n\"\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # What Is Being Evaluated\n",
    "    # --------------------------------------------------\n",
    "    lines.append(\"\\n## What Is Being Evaluated?\\n\")\n",
    "    lines.append(\n",
    "        \"TrustifAI assigns a **trust score between 0 and 1** to each answer.\\n\\n\"\n",
    "        \"- **High score** → Reliable Answer\\n\"\n",
    "        \"- **Moderate Score** → Acceptable answer (with caution)\\n\"\n",
    "        \"- **Low score** → Unreliable (Likely Hallucinated) Answer\\n\\n\"\n",
    "        \"We evaluate TrustifAI on:\\n\"\n",
    "        \"1. **LLM-generated answers**\\n\"\n",
    "        \"2. **Ground-truth answers** (known to be correct)\\n\\n\"\n",
    "        \"**Expected behavior:** Ground-truth answers should consistently receive \"\n",
    "        \"higher trust scores than LLM answers.\\n\"\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Hallucination Detection\n",
    "    # --------------------------------------------------\n",
    "    lines.append(\"\\n## Hallucination Detection (Binary Classification)\\n\")\n",
    "    lines.append(\n",
    "        \"Labels are mapped as:\\n\"\n",
    "        \"- **Trustworthy (1)** → RELIABLE, ACCEPTABLE (WITH CAUTION)\\n\"\n",
    "        \"- **Untrustworthy (0)** → UNRELIABLE\\n\\n\"\n",
    "        \"**Interpretation:**\\n\"\n",
    "        \"- ROC-AUC → separability between trustworthy vs untrustworthy answers\\n\"\n",
    "        \"- PR-AUC → robustness under class imbalance\\n\\n\"\n",
    "        \"**Results:**\\n\"\n",
    "        \"```text\\n\"\n",
    "        f\"ROC-AUC  : {fmt(metrics.get('response_roc_auc'))}\\n\"\n",
    "        f\"PR-AUC   : {fmt(metrics.get('response_pr_auc'))}\\n\"\n",
    "        \"```\"\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Score Calibration\n",
    "    # --------------------------------------------------\n",
    "    lines.append(\"\\n## Score Calibration (Ordinal Consistency)\\n\")\n",
    "    lines.append(\n",
    "        \"Ordinal labels:\\n\"\n",
    "        \"- UNRELIABLE = 0\\n\"\n",
    "        \"- ACCEPTABLE (WITH CAUTION) = 1\\n\"\n",
    "        \"- RELIABLE = 2\\n\\n\"\n",
    "        \"**Interpretation:**\\n\"\n",
    "        \"- Spearman → Monotonic ordering:\\n If answers labeled RELIABLE always score higher than ACCEPTABLE, and those score higher than UNRELIABLE, Spearman will be high.\\n\"\n",
    "        \"- Pearson → Linear calibration strength:\\n A one-step increase in label (e.g., UNRELIABLE → ACCEPTABLE) should correspond to a proportional increase in score.\\n\\n\"\n",
    "        \"**Results:**\\n\"\n",
    "        \"```text\\n\"\n",
    "        f\"Spearman : {fmt(metrics.get('response_spearman'))}\\n\"\n",
    "        f\"Pearson  : {fmt(metrics.get('response_pearson'))}\\n\"\n",
    "        \"```\"\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Reliability Distribution\n",
    "    # --------------------------------------------------\n",
    "    lines.append(\"\\n## Reliability Distribution Comparison\\n\")\n",
    "    lines.append(\n",
    "        \"A healthy system should assign:\\n\"\n",
    "        \"- More **RELIABLE** labels to **Ground Truth**\\n\"\n",
    "        \"- More **UNRELIABLE** labels to **LLM answers**\\n\\n\"\n",
    "    )\n",
    "    lines.append(\"**Results:**\\n\")\n",
    "\n",
    "    dist = metrics.get(\"label_distribution\")\n",
    "    if dist is not None:\n",
    "        lines.append(\"```text\")\n",
    "        lines.append(dist.to_string())\n",
    "        lines.append(\"```\")\n",
    "    else:\n",
    "        lines.append(\"```text\\nDistribution table not available.\\n```\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Verdict\n",
    "    # --------------------------------------------------\n",
    "    lines.append(\n",
    "        \"\\n## Verdict\\n\\n\"\n",
    "        \"TrustifAI demonstrates **meaningful separation** between grounded and \"\n",
    "        \"hallucinated answers. Ground-truth responses consistently receive \"\n",
    "        \"higher trust scores, indicating:\\n\\n\"\n",
    "        \"- Effective hallucination detection\\n\"\n",
    "        \"- Reasonable score calibration\\n\"\n",
    "        \"- Practical usefulness in RAG evaluation pipelines\\n\"\n",
    "    )\n",
    "\n",
    "    report = \"\\n\".join(lines)\n",
    "\n",
    "    with open(export_path, \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    print(\"Benchmark report exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a3169ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = compute_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5981dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_report(metric_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
